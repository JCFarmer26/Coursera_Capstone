Coursera Capstone Project
Battle of the Neighborhoods
Jordan Farmer

Foreword

	Hello reader! Thank you for taking the time to review my work on this conclusionary project to IBM’s Data Science Professional Certificate program. I started this course on April 7th of 2020 after the Coronavirus hit and I lost my job. I decided that my position within a restaurant did not provide the income, lifestyle, and future that I wanted for myself and my future family. From that morning, it was my goal to complete this course by the time the May 15th stay at home orders were lifted. With no experience with any programming language at all, this course proved to be extremely difficult! However, it has been one of the most rewarding experiences of my adult life. I have learned some extremely valuable skills that I hope to use in a professional environment after the completion of this course. I greatly appreciate your review of my work!

Introduction
	
  In this project, I was asked to use a set of data located on Wikipedia and combine it with data obtained using an API (Application Program Interface). The Wikipedia page contained the zip codes of Toronto along with their corresponding neighborhoods. The data we needed to combine it with was located on Foursquare, a social networking platform that allows users to search and review local businesses and attractions in their cities. FourSquare uses data input by users to populate their application with tons of data that visitors to a city can then access and use to their own benefit. FourSquare has a Developer API that can be used in Python to receive data and build datasets from. Once the data from FourSquare was acquired, the data could be merged with our Toronto zip code dataframe to build a geographical map of FourSquare locations in Toronto neighborhoods. From this dataframe, we were able to cluster our neighborhoods based on likenesses and proximity to each other. We were then able to build a business concept for a new business owner in Toronto by comparing reviews from the top rated coffee shops in Toronto. 
The Problem
	A local businessman needs help establishing a concept for his new coffee shop. He needs to know what the top rated coffee shops are doing right so he can reproduce their results with a more keen sense of what customers want in a coffee shop. The big business coffee retailers need some strong local competition! The most personal way we can observe the atmosphere of a coffee shop is through reviews and tips that local customers have left about businesses. Numbers and charts are great but can’t put actual customer words into box plots, or can they? Our client is asking for a crystal clear visual of what customers want from their local coffeeshop.

Data acquisition and Cleaning

The first set of data was located on a Wikipedia site that gave the zip codes and their corresponding neighborhoods of Toronto. In order to “scrape” the data into a usable python data frame, a web scraping library named Beautiful Soup was needed. The data set had several completely missing, not assigned, and duplicate inputs. Once the data was cleaned to a workable level, a second data set was introduced. This data set gave the geospatial coordinates (latitude and longitude) of the neighborhoods of Toronto. The two data sets shared a common key of “Postal code” and could be merged on “Postal Code” since they shared the same values. The resulting data frame contained the postal code, neighborhood name, and coordinates of each neighborhood. This data frame was then used in conjugation with map making library named Folium. This library will take all of our neighborhoods and map them on a satellite view of map of Toronto. This map is valuable for our client to visualize where the neighborhoods are located. The third and most crucial data source to our project was FourSquare’s Developer API. This API prepares searches based on the criteria needed and calls the application and asks for the search criteria to be met, then returns the results. The simplest analogy would be as follows:
•	Preparing to ask a very important question over the phone
•	Calling the person to ask the very important question
•	The person searches their mind for the right answer
•	They give their answer
•	We receive the answer and learn from it

We can relate this to asking somebody out on a date! But instead of spending hundreds of dollars on a date, we get very valuable data in return! Our call requires a client id, client secret, version date, latitude and longitude of Toronto, the radius of our search, and how many results we want back. Sample API call:
'https://api.foursquare.com/v2/venues/explore?&client_id=GME5MSLHY0MRGQ42F1B2330DBM2CY4B2FTCY12OBVCYERREY&client_secret=FLADA2ESTV0UC5UIUQXOCASDUOLLLY2TDAHEFL1DBI1UCJU5&v=20180605&ll=43.685347,-79.3381065&radius=1000&limit=20' 
The results received were in the form of a json file which we then synthesized into a workable dataframe. The data given was cross referenced with our neighborhoods dataframe to map our results into our previous Toronto map. 

Data Exploration and Analysis
	
  The FourSquare API is a very valuable resource. The call made just on Toronto as a whole gave hundreds of venues with numerous categories including American Restaurant, Turkish Restaurant, Strip Club, Coffee, Airport Foodcourt, etc. To get a generalization of type of neighborhood each venue was in, a top 5 venue list was created to show the five most visited venues in each neighborhood. This information allowed us to get an idea of what type of neighborhood the venues were in. Each column contained the category of venue and how often they appeared in different neighborhoods. This information combined with the information we already had about neighborhoods and their locations allowed us to view specific venues located in specific neighborhoods and plot them visually. To plot them visually, they were clustered and grouped together by similarity and frequency of appearance per neighborhood. These clusters were then plotted on our map of Toronto from before. By calling for a specific category of venue,  a data set can be obtained containing information about specific venues such as location, unique ID, tips, menus, users who have been there, and, most importantly to our client, user reviews.  The venue Coffee was specified in the next API call and we received a json file that needed to be converted into a workable data frame. 
In order to get the reviews from each location, the unique ID (last column) is needed to inquire FourSquare about each individual locations reviews. Once the call is prepared and made specifying an individual venue, two tips (reviews) were received at the most from each call. Unfortunately, FourSquare only allows two reviews to be returned if you aren’t willing to pay exorbitant amounts of money for more results.  Example of a tip received from our call on a specific coffee venue:
	'text': "Tried the capuccino (on the left) - was smoother & less burnt tasting than most others I've been to but not the best. Owner was really friendly though and look at that amazing latte art! Try it out :)",

Results
	
  The process of calling on unique coffee venues to obtain their tips two at a time was repeated time and time and a text document was compiled containing every review for each coffee shop called on. The resulting text document contains nothing but strings of strings of non numerical data. Our client needed a visual representation of what paying customers are saying the most about the more popular coffee shops in the Toronto area. This is where I had the idea for a word cloud visual. 
	
  A Word Cloud is a powerful visualization tool for text data. The cloud contains the words from a text document that appear most often. How often they appear is represented by the size of the word in the cloud. If a word appears two hundred times in a document, the word would be much larger in the cloud and taking up more room than a word that appeared only fifty times in the same document. Our text document containing reviews contained over six thousand words and an algorithm was run to count the frequencies of words while ignoring Stopwords. Stopwords are words that the algorithm and/or user can specify that should NOT be represented in the word cloud and should therefore not be counted. The data that was obtained showed the most common words that appeared in the text document and how many times they appeared. Some example common Stopwords are: a, the, well, be, I , in, etc. This feature ensures that we only receive relevant words such as nouns, verbs, and adjectives. The included stopwords certainly cut down on time needed to iterate through the entire text but we can add even more stopwords to the list to refine our results. Through several refinements of the stopwords and adjustments to the plot settings (colors, shape, font size, etc.), the final wordcloud shows the most relevant, valuable, and conceptually integral words that pertain to a successful coffeehouse.
	
 Discussion/Conclusion
	
  First, I should preface with the fact that I have been in the restaurant industry for more than half of my life. It has been my job to use words to describe very expensive food and wine to very wealthy individuals in some of the highest accoladed restaurants in California. I have developed quite a strong and descriptive vocabulary as well as a strong sense of personalization when it comes to guest/client needs. I, along with many of you,  have been jobless for months and I genuinely miss the social interaction and meeting new people every single day. My goal with this project was to use data science techniques to still be able to make a personal connection to people through words. 
	The wordcloud above makes it very easy to see what the most common words in a body of text are but it goes beyond that. It took real user input. A real connection to a consumer who has traveled to, spent money at, and felt the need to review, a business in Toronto. Hundreds of user inputs were aggregated into one body of text and then manipulated into a valuable display. The stakeholders, designers, and owners of our soon-to-be coffee shop can extrapolate this data and begin to plan the environment of their business locations. 
